---
title: "Lab 10 - RF, XGBoost"
output: 
  html_document: default
  # tufte::tufte_html:
  #   css: style.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include  = T)
```

# Learning goals

- Perform classification and regression with tree-based methods in R
- Recognize that tree-based methods are capable of capturing non-linearities by splitting multiple times on the same variables
- Compare the performance of classification trees, bagging, random forests, and boosting for predicting heart disease based on the ``heart`` data.

# Lab description

For this lab we will be working with simulated data and the `heart` dataset that you can download from [here](https://github.com/JSC370/jsc370-2023/blob/main/data/heart/heart.csv)


### Setup packages

You should install and load `rpart` (trees), `randomForest` (random forest), `gbm` (gradient boosting) and `xgboost` (extreme gradient boosting).


```{r, eval=FALSE, warning=FALSE}
install.packages(c("rpart", "rpart.plot", "randomForest", "gbm", "xgboost"))
```

### Load packages and data

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(xgboost)
heart <- read.csv("https://raw.githubusercontent.com/JSC370/jsc370-2023/main/data/heart/heart.csv") |>
  mutate(
    AHD = 1 * (AHD == "Yes"),
    ChestPain = factor(ChestPain),
    Thal = factor(Thal)
  )
head(heart)
```


---

# Questions

## Question 1: Trees with simulated data

- Simulate data from a random uniform distribution [-5,5] and normally distributed errors (s.d = 0.5)
- Create a non-linear relationship y=sin(x)+error
- Split the data into test and training sets (500 points each), plot the data

$$Y_1 = \sin(X_1) + \varepsilon_1$$
$$\varepsilon_1 \sim N(0, 0.5^2)$$
for $i = 1, \dots 1000$.

```{r sim,   warning=FALSE}
set.seed(1984)
n <- 1000
x <- runif(n, -5,5) 
error <- rnorm(n, sd = 0.5)
y <- sin(x) + error 
nonlin <- data.frame(y = y, x = x)
train_size <- sample(1:1000, size = 500)
nonlin_train <- nonlin[train_size,]
nonlin_test <- nonlin[-train_size,]
ggplot(nonlin,aes(y=y,x=x))+
  geom_point() +
  theme_minimal()
```

- Fit a regression tree using the training set, plot it

```{r tree, warning=FALSE}
treefit <- rpart(y ~ x, data = nonlin_train, method = "anova", control = list(cp = 0))
rpart.plot(treefit)
```

- Determine the optimal complexity parameter (cp) to prune the tree

```{r, warning=FALSE}
plotcp(treefit)
cp_summary <- printcp(treefit)
optimal_cp <- cp_summary[9, 1]
```

- Plot the pruned tree and summarize

```{r, warning=FALSE}
tree_pruned <- prune(treefit, cp = optimal_cp)
rpart.plot(tree_pruned)
summary(tree_pruned)
```

- Based on the plot and/or summary of the pruned tree create a vector of the (ordered) split points for variable x, and a vector of fitted values for the intervals determined by the split points of x.

```{r, warning=FALSE}
x_splits <-  sort(tree_pruned$splits[ , "index"])
# y_frame <- tree_pruned$frame
# y_splits <- y_frame |>
#   filter(var == '<leaf>') |>
#   pull(yval)

predict(tree_pruned, data.frame(x = x_splits))
```

```{r, warning=FALSE}
stpfn <- stepfun(x_splits, predict(tree_pruned, data.frame(x = c(-999, x_splits))))
plot(y ~ x, data = nonlin_train)
plot(stpfn, add = TRUE, lwd = 2, col = 'pink')
```

- Fit a linear model to the training data and plot the regression line. 
```{r, warning=FALSE}
lmfit <- lm(y ~ x, data = nonlin_train)
plot(y ~ x, data = nonlin_train)
abline(lmfit, col = 'darkgreen', lwd = 2)
plot(stpfn, add = TRUE, lwd = 2, col = 'pink')
```

- Contrast the quality of the fit of the tree model vs. linear regression by inspection of the plot

The trend predicted by tree model is more consistent with the real trend than linear regression

- Compute the test MSE of the pruned tree and the linear regression model
$$MSE = \frac{1}{n} \sum_{i=1}^n ...$$
```{r, warning=FALSE}
tree_pred <- predict(tree_pruned, nonlin_test)
lm_pred <- predict(lmfit, nonlin_test)
tibble(tree_pred, lm_pred, y = nonlin_test$y) |>
  summarise(
    tree_mse =  sum((tree_pred - y)^2) / n(),
    lm_mse = sum((lm_pred - y)^2 / n())
  )
```

- Is the lm or regression tree better at fitting a non-linear function?

tree_mse is much smaller than lm_mse, so tree is better.

---

## Question 2: Analysis of Real Data
- Split the `heart` data into training and testing (70-30%)
```{r real_tree}
set.seed(2023)
train <- sample(1:nrow(heart), round(0.7*nrow(heart)))
heart_train <- heart[train,]
heart_test <- heart[-train,]
```

- Fit a classification tree using rpart, plot the full tree
```{r}
heart_tree <- rpart(
  AHD ~., data = heart_train,
  method = "class",
  control = list(minsplits = 10, minbucket = 3, cp = 0, xval = 10)
)
rpart.plot(heart_tree)
```

- Plot the complexity parameter table for an rpart fit and prune the tree
```{r}
plotcp(heart_tree)
cp_summary <- printcp(heart_tree)
optimal_cp <- cp_summary[2, 1]
```

- Compute the test misclassification error
$$Err = \frac{1}{n}\sum_{i=1}^n I\left(y_i \neq y \right)$$
```{r}
heart_pred <- predict(heart_tree, heart_test)
sum((heart_pred[ , 2] > 0.5) == (heart_test$AHD == 0)) / nrow(heart_test)
```

- Fit the tree with the optimal complexity parameter to the full data (training + testing)
```{r}
heart_tree_full <- rpart(
  AHD ~., data = heart,
  method = "class",
  control = list(minsplits = 10, minbucket = 3, cp = 0, xval = 10)
)
heart_tree_pruned <- prune(heart_tree_full, cp = optimal_cp)
rpart.plot(heart_tree_pruned)
summary(heart_tree_pruned)
```

---

## Question 3: Bagging, Random Forest

- Compare the performance of classification trees (above), bagging, random forests for predicting heart disease based on the ``heart`` data.


- Split the data into training and testing. Train each of the models on the training data and extract the cross-validation (or out-of-bag error for bagging and Random forest). 
```{r}
set.seed(2023)
train <- sample(1:nrow(heart), round(0.7 * nrow(heart)))
heart_train <- heart[train, ]
heart_test <- heart[-train, ]
```

- For bagging use ``randomForest`` with ``mtry`` equal to the number of features (all other parameters at their default values). Generate the variable importance plot using ``varImpPlot`` and extract variable importance from the ``randomForest`` fitted object using the ``importance`` function.
```{r}
n_features <- dim(heart_train)[2] - 1
heart_bagging <- randomForest(as.factor(AHD) ~ ., data = heart_train, mtry = n_features, na.action = na.omit)
mean(heart_bagging$err.rate[, 1])

varImpPlot(heart_bagging, main = "Variable Importance plot")

importance(heart_bagging)
```

- For random forests use ``randomForest`` with the default parameters. Generate the variable importance plot using ``varImpPlot`` and extract variable importance from the ``randomForest`` fitted object using the ``importance`` function.

```{r}
floor(sqrt(n_features))
heart_forest <- randomForest(as.factor(AHD) ~ .,
                             data = heart_train,
                             na.action = na.omit)

mean(heart_forest$err.rate[, 1])

varImpPlot(heart_forest, main = "Variable Importance plot")

importance(heart_forest)
```

---

## Question 4: Boosting

- For boosting use `gbm` with ``cv.folds=5`` to perform 5-fold cross-validation, and set ``class.stratify.cv`` to ``AHD`` (heart disease outcome) so that cross-validation is performed stratifying by ``AHD``.  Plot the cross-validation error as a function of the boosting iteration/trees (the `$cv.error` component of the object returned by ``gbm``) and determine whether additional boosting iterations are warranted. If so, run additional iterations with  ``gbm.more`` (use the R help to check its syntax). Choose the optimal number of iterations. Use the ``summary.gbm`` function to generate the variable importance plot and extract variable importance/influence (``summary.gbm`` does both). Generate 1D and 2D marginal plots with ``gbm.plot`` to assess the effect of the top three variables and their 2-way interactions. 

```{r}
heart_boost <- gbm(
  AHD ~. , data = heart_train, 
  distribution = "bernoulli",
  cv.folds = 5, class.stratify.cv = TRUE,
  n.trees = 3000
)
plot(heart_boost$cv.error, type = 'l', ylim = c(0, 2), lwd = 3, col = "darkgreen")
lines(heart_boost$train.error, lwd = 2, col = "pink")

```

---


# Deliverables

1. Questions 1-4 answered, pdf or html output uploaded to quercus